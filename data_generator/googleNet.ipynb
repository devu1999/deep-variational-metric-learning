{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'chainer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-64702289ef21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0m_import_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mchainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconcat_examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrelu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'chainer'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "\n",
    "import numpy\n",
    "try:\n",
    "    from PIL import Image\n",
    "    available = True\n",
    "except ImportError as e:\n",
    "    available = False\n",
    "    _import_error = e\n",
    "\n",
    "from chainer.dataset.convert import concat_examples\n",
    "from chainer.dataset import download\n",
    "from chainer.functions.activation.relu import relu\n",
    "from chainer.functions.activation.softmax import softmax\n",
    "from chainer.functions.array.reshape import reshape\n",
    "from chainer.functions.math.sum import sum\n",
    "from chainer.functions.pooling.average_pooling_2d import average_pooling_2d\n",
    "from chainer.functions.pooling.max_pooling_2d import max_pooling_2d\n",
    "from chainer.functions.normalization.local_response_normalization import (\n",
    "    local_response_normalization)\n",
    "from chainer.functions.noise.dropout import dropout\n",
    "from chainer.initializers import constant\n",
    "from chainer.initializers import uniform\n",
    "from chainer import link\n",
    "from chainer.links.connection.convolution_2d import Convolution2D\n",
    "from chainer.links.connection.inception import Inception\n",
    "from chainer.links.connection.linear import Linear\n",
    "from chainer.serializers import npz\n",
    "from chainer.utils import imgproc\n",
    "from chainer.variable import Variable\n",
    "\n",
    "\n",
    "class GoogLeNet(link.Chain):\n",
    "\n",
    "    image_mean = numpy.array([104, 117, 123], dtype=numpy.float32)  # BGR\n",
    "\n",
    "    def __init__(self, pretrained_model='auto'):\n",
    "        if pretrained_model:\n",
    "            # As a sampling process is time-consuming,\n",
    "            # we employ a zero initializer for faster computation.\n",
    "            kwargs = {'initialW': constant.Zero()}\n",
    "        else:\n",
    "            # employ default initializers used in the original paper\n",
    "            kwargs = {'initialW': uniform.GlorotUniform(scale=1.0)}\n",
    "        super(GoogLeNet, self).__init__(\n",
    "            conv1=Convolution2D(3,  64, 7, stride=2, pad=3, **kwargs),\n",
    "            conv2_reduce=Convolution2D(64,  64, 1, **kwargs),\n",
    "            conv2=Convolution2D(64, 192, 3, stride=1, pad=1, **kwargs),\n",
    "            inc3a=Inception(192,  64,  96, 128, 16,  32,  32),\n",
    "            inc3b=Inception(256, 128, 128, 192, 32,  96,  64),\n",
    "            inc4a=Inception(480, 192,  96, 208, 16,  48,  64),\n",
    "            inc4b=Inception(512, 160, 112, 224, 24,  64,  64),\n",
    "            inc4c=Inception(512, 128, 128, 256, 24,  64,  64),\n",
    "            inc4d=Inception(512, 112, 144, 288, 32,  64,  64),\n",
    "            inc4e=Inception(528, 256, 160, 320, 32, 128, 128),\n",
    "            inc5a=Inception(832, 256, 160, 320, 32, 128, 128),\n",
    "            inc5b=Inception(832, 384, 192, 384, 48, 128, 128),\n",
    "            loss3_fc=Linear(1024, 1000, **kwargs),\n",
    "\n",
    "            loss1_conv=Convolution2D(512, 128, 1, **kwargs),\n",
    "            loss1_fc1=Linear(2048, 1024, **kwargs),\n",
    "            loss1_fc2=Linear(1024, 1000, **kwargs),\n",
    "\n",
    "            loss2_conv=Convolution2D(528, 128, 1, **kwargs),\n",
    "            loss2_fc1=Linear(2048, 1024, **kwargs),\n",
    "            loss2_fc2=Linear(1024, 1000, **kwargs)\n",
    "        )\n",
    "        if pretrained_model == 'auto':\n",
    "            _retrieve(\n",
    "                'bvlc_googlenet.npz',\n",
    "                'http://dl.caffe.berkeleyvision.org/bvlc_googlenet.caffemodel',\n",
    "                self)\n",
    "        elif pretrained_model:\n",
    "            npz.load_npz(pretrained_model, self)\n",
    "        self.functions = OrderedDict([\n",
    "            ('conv1', [self.conv1, relu]),\n",
    "            ('pool1', [_max_pooling_2d, _local_response_normalization]),\n",
    "            ('conv2_reduce', [self.conv2_reduce, relu]),\n",
    "            ('conv2', [self.conv2, relu, _local_response_normalization]),\n",
    "            ('pool2', [_max_pooling_2d]),\n",
    "            ('inception_3a', [self.inc3a]),\n",
    "            ('inception_3b', [self.inc3b]),\n",
    "            ('pool3', [_max_pooling_2d]),\n",
    "            ('inception_4a', [self.inc4a]),\n",
    "            ('inception_4b', [self.inc4b]),\n",
    "            ('inception_4c', [self.inc4c]),\n",
    "            ('inception_4d', [self.inc4d]),\n",
    "            ('inception_4e', [self.inc4e]),\n",
    "            ('pool4', [_max_pooling_2d]),\n",
    "            ('inception_5a', [self.inc5a]),\n",
    "            ('inception_5b', [self.inc5b]),\n",
    "            ('pool5', [_average_pooling_2d_k7]),\n",
    "            ('loss3_fc', [_dropout, self.loss3_fc]),\n",
    "            ('prob', [softmax]),\n",
    "            # Since usually the following outputs are not used, they are put\n",
    "            # after 'prob' to be skipped for efficiency.\n",
    "            ('loss1_fc2', [_average_pooling_2d_k5, self.loss1_conv, relu,\n",
    "                           self.loss1_fc1, relu, self.loss1_fc2]),\n",
    "            ('loss2_fc2', [_average_pooling_2d_k5, self.loss2_conv, relu,\n",
    "                           self.loss2_fc1, relu, self.loss2_fc2])\n",
    "        ])\n",
    "\n",
    "    @property\n",
    "    def available_layers(self):\n",
    "        return list(self.functions.keys())\n",
    "\n",
    "    @classmethod\n",
    "    def convert_caffemodel_to_npz(cls, path_caffemodel, path_npz):\n",
    "\n",
    "        # As CaffeFunction uses shortcut symbols,\n",
    "        # we import CaffeFunction here.\n",
    "        from chainer.links.caffe.caffe_function import CaffeFunction\n",
    "        caffemodel = CaffeFunction(path_caffemodel)\n",
    "        chainermodel = cls(pretrained_model=None)\n",
    "        _transfer_googlenet(caffemodel, chainermodel)\n",
    "        npz.save_npz(path_npz, chainermodel, compression=False)\n",
    "\n",
    "    def __call__(self, x, layers=['prob'], train=False):\n",
    "\n",
    "        h = x\n",
    "        activations = {}\n",
    "        inception_4a_cache = None\n",
    "        inception_4d_cache = None\n",
    "        target_layers = set(layers)\n",
    "        for key, funcs in self.functions.items():\n",
    "            if len(target_layers) == 0:\n",
    "                break\n",
    "\n",
    "            if key == 'loss1_fc2':\n",
    "                h = inception_4a_cache\n",
    "            elif key == 'loss2_fc2':\n",
    "                h = inception_4d_cache\n",
    "\n",
    "            for func in funcs:\n",
    "                h = func(h)\n",
    "\n",
    "            if key in target_layers:\n",
    "                activations[key] = h\n",
    "                target_layers.remove(key)\n",
    "\n",
    "            if key == 'inception_4a':\n",
    "                inception_4a_cache = h\n",
    "            elif key == 'inception_4d':\n",
    "                inception_4d_cache = h\n",
    "\n",
    "        return activations\n",
    "\n",
    "    def extract(self, images, layers=['pool5'], size=(224, 224)):\n",
    "\n",
    "        x = concat_examples([prepare(img, size=size) for img in images])\n",
    "        x = Variable(self.xp.asarray(x))\n",
    "        return self(x, layers=layers)\n",
    "\n",
    "    def predict(self, images, oversample=True):\n",
    "    \n",
    "\n",
    "        x = concat_examples([prepare(img, size=(256, 256)) for img in images])\n",
    "        if oversample:\n",
    "            x = imgproc.oversample(x, crop_dims=(224, 224))\n",
    "        else:\n",
    "            x = x[:, :, 16:240, 16:240]\n",
    "        # Set volatile option to ON to reduce memory consumption\n",
    "        x = Variable(self.xp.asarray(x))\n",
    "        y = self(x, layers=['prob'])['prob']\n",
    "        if oversample:\n",
    "            n = y.data.shape[0] // 10\n",
    "            y_shape = y.data.shape[1:]\n",
    "            y = reshape(y, (n, 10) + y_shape)\n",
    "            y = sum(y, axis=1) / 10\n",
    "        return y\n",
    "\n",
    "\n",
    "def prepare(image, size=(224, 224)):\n",
    "\n",
    "\n",
    "    if not available:\n",
    "        raise ImportError('PIL cannot be loaded. Install Pillow!\\n'\n",
    "                          'The actual import error is as follows:\\n' +\n",
    "                          str(_import_error))\n",
    "    if isinstance(image, numpy.ndarray):\n",
    "        if image.ndim == 3:\n",
    "            if image.shape[0] == 1:\n",
    "                image = image[0, :, :]\n",
    "            elif image.shape[0] == 3:\n",
    "                image = image.transpose((1, 2, 0))\n",
    "        image = Image.fromarray(image.astype(numpy.uint8))\n",
    "    image = image.convert('RGB')\n",
    "    if size:\n",
    "        image = image.resize(size)\n",
    "    image = numpy.asarray(image, dtype=numpy.float32)\n",
    "    image = image[:, :, ::-1]\n",
    "    image -= GoogLeNet.image_mean\n",
    "    image = image.transpose((2, 0, 1))\n",
    "    return image\n",
    "\n",
    "\n",
    "def _transfer_inception(src, dst, names):\n",
    "    for name in names:\n",
    "        chain = getattr(dst, 'inc{}'.format(name))\n",
    "        src_prefix = 'inception_{}/'.format(name)\n",
    "        chain.conv1.W.data[:] = src[src_prefix + '1x1'].W.data\n",
    "        chain.conv1.b.data[:] = src[src_prefix + '1x1'].b.data\n",
    "        chain.proj3.W.data[:] = src[src_prefix + '3x3_reduce'].W.data\n",
    "        chain.proj3.b.data[:] = src[src_prefix + '3x3_reduce'].b.data\n",
    "        chain.conv3.W.data[:] = src[src_prefix + '3x3'].W.data\n",
    "        chain.conv3.b.data[:] = src[src_prefix + '3x3'].b.data\n",
    "        chain.proj5.W.data[:] = src[src_prefix + '5x5_reduce'].W.data\n",
    "        chain.proj5.b.data[:] = src[src_prefix + '5x5_reduce'].b.data\n",
    "        chain.conv5.W.data[:] = src[src_prefix + '5x5'].W.data\n",
    "        chain.conv5.b.data[:] = src[src_prefix + '5x5'].b.data\n",
    "        chain.projp.W.data[:] = src[src_prefix + 'pool_proj'].W.data\n",
    "        chain.projp.b.data[:] = src[src_prefix + 'pool_proj'].b.data\n",
    "\n",
    "\n",
    "def _transfer_googlenet(src, dst):\n",
    "    # 1 #################################################################\n",
    "    dst.conv1.W.data[:] = src['conv1/7x7_s2'].W.data\n",
    "    dst.conv1.b.data[:] = src['conv1/7x7_s2'].b.data\n",
    "\n",
    "    # 2 #################################################################\n",
    "    dst.conv2_reduce.W.data[:] = src['conv2/3x3_reduce'].W.data\n",
    "    dst.conv2_reduce.b.data[:] = src['conv2/3x3_reduce'].b.data\n",
    "    dst.conv2.W.data[:] = src['conv2/3x3'].W.data\n",
    "    dst.conv2.b.data[:] = src['conv2/3x3'].b.data\n",
    "\n",
    "    # 3, 4, 5 ###########################################################\n",
    "    _transfer_inception(src, dst, ['3a', '3b',\n",
    "                                   '4a', '4b', '4c', '4d', '4e',\n",
    "                                   '5a', '5b'])\n",
    "\n",
    "    # outputs ############################################################\n",
    "    dst.loss1_conv.W.data[:] = src['loss1/conv'].W.data\n",
    "    dst.loss1_conv.b.data[:] = src['loss1/conv'].b.data\n",
    "    dst.loss1_fc1.W.data[:] = src['loss1/fc'].W.data\n",
    "    dst.loss1_fc1.b.data[:] = src['loss1/fc'].b.data\n",
    "    dst.loss1_fc2.W.data[:] = src['loss1/classifier'].W.data\n",
    "    dst.loss1_fc2.b.data[:] = src['loss1/classifier'].b.data\n",
    "\n",
    "    dst.loss2_conv.W.data[:] = src['loss2/conv'].W.data\n",
    "    dst.loss2_conv.b.data[:] = src['loss2/conv'].b.data\n",
    "    dst.loss2_fc1.W.data[:] = src['loss2/fc'].W.data\n",
    "    dst.loss2_fc1.b.data[:] = src['loss2/fc'].b.data\n",
    "    dst.loss2_fc2.W.data[:] = src['loss2/classifier'].W.data\n",
    "    dst.loss2_fc2.b.data[:] = src['loss2/classifier'].b.data\n",
    "\n",
    "    dst.loss3_fc.W.data[:] = src['loss3/classifier'].W.data\n",
    "    dst.loss3_fc.b.data[:] = src['loss3/classifier'].b.data\n",
    "\n",
    "\n",
    "def _max_pooling_2d(x):\n",
    "    return max_pooling_2d(x, ksize=3, stride=2)\n",
    "\n",
    "\n",
    "def _local_response_normalization(x):\n",
    "    return local_response_normalization(x, n=5, k=1, alpha=1e-4/5)\n",
    "\n",
    "\n",
    "def _average_pooling_2d_k5(x):\n",
    "    return average_pooling_2d(x, ksize=5, stride=3)\n",
    "\n",
    "\n",
    "def _average_pooling_2d_k7(x):\n",
    "    return average_pooling_2d(x, ksize=7, stride=1)\n",
    "\n",
    "\n",
    "def _dropout(x):\n",
    "    return dropout(x, ratio=0.4)\n",
    "\n",
    "\n",
    "def _make_npz(path_npz, url, model):\n",
    "    path_caffemodel = download.cached_download(url)\n",
    "    print('Now loading caffemodel (usually it may take few minutes)')\n",
    "    GoogLeNet.convert_caffemodel_to_npz(path_caffemodel, path_npz)\n",
    "    npz.load_npz(path_npz, model)\n",
    "    return model\n",
    "\n",
    "\n",
    "def _retrieve(name_npz, url, model):\n",
    "    root = download.get_dataset_directory('pfnet/chainer/models/')\n",
    "    path = os.path.join(root, name_npz)\n",
    "    return download.cache_or_load_file(\n",
    "        path, lambda path: _make_npz(path, url, model),\n",
    "        lambda path: npz.load_npz(path, model))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import matplotlib.pyplot as plt\n",
    "    from synset_words import get_synset_words\n",
    "\n",
    "    images = []\n",
    "    for filename in os.listdir('ILSVRC2012'):\n",
    "        images.append(plt.imread(os.path.join('ILSVRC2012', filename)))\n",
    "        break  # TODO: Remove this line\n",
    "\n",
    "    model = GoogLeNet()\n",
    "    y = model.predict(images)\n",
    "\n",
    "    top_10 = numpy.argsort(y.data[0])[:-10:-1]\n",
    "    synset_words = get_synset_words()\n",
    "    for i in top_10:\n",
    "        print(y.data[0][i], synset_words[i][1])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
